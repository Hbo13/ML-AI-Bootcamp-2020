{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of B1.3.3 - Tensorflow + tensorboard initiation - teachers' sheet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu16Psl6Bmzc",
        "outputId": "7ffd8aa3-3018-4e3e-c51c-183cde67abe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtMdAihRBnzu"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import strftime\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YueLFzrlCRIz"
      },
      "source": [
        "NR_CLASSES = 10\n",
        "VALIDATION_SIZE = 10000\n",
        "IMAGE_WIDTH = 28\n",
        "IMAGE_HEIGHT = 28\n",
        "CHANNELS = 1\n",
        "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwDC3cvHDkqz"
      },
      "source": [
        "### 1) Download mnist dataset and create train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0XFWCtzBtZD",
        "outputId": "523e962c-0390-4bfc-cd7a-d6a62b81346f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjZr4NWFD_9M"
      },
      "source": [
        "### 2) Reshape and rescale data: make it between 0 and 1 - neural networs usually work better with this type of data, do one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu__ahAsCADX",
        "outputId": "2fc6cd1f-d1f6-41bd-b92a-e7e34022810c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "nsamples, nx, ny = X_train.shape\n",
        "nsamples2, nx2, ny2 = X_test.shape\n",
        "x_train_all = X_train.reshape((nsamples, TOTAL_INPUTS))\n",
        "x_test = X_test.reshape((nsamples2, TOTAL_INPUTS))\n",
        "\n",
        "# Re-scale\n",
        "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0\n",
        "y_train_all = np.eye(NR_CLASSES)[Y_train]\n",
        "y_test = np.eye(NR_CLASSES)[Y_test]\n",
        "\n",
        "print(y_train_all.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 10)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqdkZKoMDc-N"
      },
      "source": [
        "### 3) Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: x_val, y_val, x_train, and y_train from x_train_all and y_train_all. Use the validation size of 10,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkVEb4UDeyJ",
        "outputId": "8dccf782-42cd-4d25-c442-427d19d36bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_val = x_train_all[:VALIDATION_SIZE]\n",
        "y_val = y_train_all[:VALIDATION_SIZE]\n",
        "\n",
        "x_train = x_train_all[VALIDATION_SIZE:]\n",
        "y_train = y_train_all[VALIDATION_SIZE:]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-043Q2iEyFE"
      },
      "source": [
        "### 4) Setup Tensorflow Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDq78OMEdZV",
        "outputId": "24caa3d4-5484-44a6-e1ad-c607cdb52148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
        "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZBtcVXBFCvp"
      },
      "source": [
        "### 5) Create variables for number of epochs, learning rate and two hidden layers: 512 and 64 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foCD_-ejEwjr"
      },
      "source": [
        "#nr_epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "n_hidden1 = 512\n",
        "n_hidden2 = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jzIC73bG4pG"
      },
      "source": [
        "# Setup of tensorboard on google colab\n",
        "Tensorboard is the very good way to visualise your data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KagdMxH4GNAH",
        "outputId": "bcd7de24-9a37-4d0e-efcb-c5c26ffc6d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-05 08:36:44--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.20.12.96, 34.197.46.159, 50.17.165.171, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.20.12.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  17.1MB/s    in 0.8s    \n",
            "\n",
            "2019-11-05 08:36:45 (17.1 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "http://b536d46e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF-KGlt8FMO-"
      },
      "source": [
        "### 6) Create function to proceed one layer in neural network:\n",
        "You have input, dimension of weight: weight_dim, dimension of bias: bias_dim and name of your layer, return the output layer.\n",
        "Use truncated normal distribution to generate initial weights and zero-constants for biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-3yRrdgFRA8"
      },
      "source": [
        "def setup_layer(input, weight_dim, bias_dim, name):\n",
        "    \n",
        "    with tf.name_scope(name):\n",
        "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
        "        w = tf.Variable(initial_value=initial_w, name='W')\n",
        "\n",
        "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
        "        b = tf.Variable(initial_value=initial_b, name='B')\n",
        "\n",
        "        layer_in = tf.matmul(input, w) + b\n",
        "        \n",
        "        if name=='out':\n",
        "            layer_out = tf.nn.softmax(layer_in)\n",
        "        else:\n",
        "            layer_out = tf.nn.relu(layer_in)\n",
        "        \n",
        "        tf.summary.histogram('weights', w)\n",
        "        tf.summary.histogram('biases', b)\n",
        "        \n",
        "        return layer_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWMKGrW4Fx6G"
      },
      "source": [
        "### 7) Create neural network with 2 hidden layers, using this function from previous item. Add also one dropout layer to avoid overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lesgVnOkFvM3"
      },
      "source": [
        "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
        "                      bias_dim=[n_hidden1], name='layer_1')\n",
        "\n",
        "layer_drop = tf.nn.dropout(layer_1, rate=0.2, name='dropout_layer')\n",
        "\n",
        "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
        "                      bias_dim=[n_hidden2], name='layer_2')\n",
        "\n",
        "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
        "                      bias_dim=[NR_CLASSES], name='out')\n",
        "\n",
        "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RYxCatJNqZQ"
      },
      "source": [
        "### 8) For better visualization in Tensorboard we want to use tf.name_scope() to aggregate loss, optimizer, accuracy metrica and performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejuDsIObGgyH"
      },
      "source": [
        "# Defining Loss Function\n",
        "with tf.name_scope('loss_calc'):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))\n",
        "# Defining Optimizer\n",
        "with tf.name_scope('optimizer'):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    train_step = optimizer.minimize(loss)\n",
        "# Accuracy Metric\n",
        "with tf.name_scope('accuracy_calc'):\n",
        "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "with tf.name_scope('performance'):\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    tf.summary.scalar('cost', loss)\n",
        "\n",
        "#Check Input Images in Tensorboard\n",
        "\n",
        "with tf.name_scope('show_image'):\n",
        "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
        "    tf.summary.image('image_input', x_image, max_outputs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxxIfeCkOYmb"
      },
      "source": [
        "### 9) Create session using tf.Session(), merge summaries using tf.summary.merge_all(). Use tf.summary.FileWriter() to write you summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0_Kp5PqHZyn"
      },
      "source": [
        "#Run Session\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "merged_summary = tf.summary.merge_all()\n",
        "\n",
        "train_writer = tf.summary.FileWriter('./log/train')\n",
        "train_writer.add_graph(sess.graph)\n",
        "\n",
        "validation_writer = train_writer = tf.summary.FileWriter('./log/validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOWynWYPEL4"
      },
      "source": [
        "### 10) Initialise all the variables, and run the session, look at the TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcc-SvdCIvev"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs2U22flPSS4"
      },
      "source": [
        "### 11) If you data is quite big, it is usefull to have so-called batches, smaller pieces of data. We have 50000 data points, we want to have batches with 1000 points. Create next_batch function, which gives you the part of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VqiQFnmRXKg",
        "outputId": "c87a00b2-8f00-4a56-fcad-1b9b75e0838b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "size_of_batch = 1000\n",
        "num_examples = y_train.shape[0]\n",
        "nr_iterations = int(num_examples/size_of_batch)\n",
        "\n",
        "index_in_epoch = 0\n",
        "print(\"num_examples =\", num_examples)\n",
        "print(\"nr_iterations =\", nr_iterations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_examples = 50000\n",
            "nr_iterations = 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaQE1pAJJIxE"
      },
      "source": [
        "def next_batch(batch_size, data, labels):\n",
        "    \n",
        "    global num_examples\n",
        "    global index_in_epoch\n",
        "    \n",
        "    start = index_in_epoch\n",
        "    index_in_epoch += batch_size\n",
        "    \n",
        "    if index_in_epoch > num_examples:\n",
        "        start = 0\n",
        "        index_in_epoch = batch_size\n",
        "    \n",
        "    end = index_in_epoch\n",
        "    \n",
        "    return data[start:end], labels[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TQ6jPVgHZiA",
        "outputId": "0356e6d6-c7cf-4202-f866-e1b98e4a04af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "batch_x, batch_y = next_batch(size_of_batch, x_train, y_train)\n",
        "print(batch_x.shape)\n",
        "print(batch_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 784)\n",
            "(1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMACFD2yPh7w"
      },
      "source": [
        "### 12) Run the algorighm: do several so-called epochs - the runs through all the data. In each epoch use 50 batches with 1000 data points. Write information to TensorBoard to investigate later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VVfYfotJKRU",
        "outputId": "370bbe04-c71f-4792-9078-541248060b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "nr_epochs = 10\n",
        "\n",
        "for epoch in range(nr_epochs):\n",
        "    \n",
        "    # ============= Training Dataset =========\n",
        "    for i in range(nr_iterations):\n",
        "        \n",
        "        batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
        "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
        "        sess.run(train_step, feed_dict=feed_dictionary)\n",
        "        \n",
        "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)  \n",
        "    train_writer.add_summary(s, epoch)\n",
        "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
        "    \n",
        "    # ================== Validation ======================\n",
        "    \n",
        "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
        "    validation_writer.add_summary(summary, epoch)\n",
        "\n",
        "print('Done training!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t| Training Accuracy = 0.9089999794960022\n",
            "Epoch 1 \t| Training Accuracy = 0.9399999976158142\n",
            "Epoch 2 \t| Training Accuracy = 0.9559999704360962\n",
            "Epoch 3 \t| Training Accuracy = 0.9580000042915344\n",
            "Epoch 4 \t| Training Accuracy = 0.9620000123977661\n",
            "Epoch 5 \t| Training Accuracy = 0.9670000076293945\n",
            "Epoch 6 \t| Training Accuracy = 0.9639999866485596\n",
            "Epoch 7 \t| Training Accuracy = 0.9710000157356262\n",
            "Epoch 8 \t| Training Accuracy = 0.9729999899864197\n",
            "Epoch 9 \t| Training Accuracy = 0.9760000109672546\n",
            "Done training!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kTrt30KSRz-"
      },
      "source": [
        "### 13) Calculate the accuracy over the test dataset (x_test and y_test). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-oNL6CWLXT5",
        "outputId": "74a393cf-8d08-4a1e-94f4-ab617521524b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracy = sess.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
        "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set is 96.64%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYZVjiVZSYD3"
      },
      "source": [
        "### 14) Reset for the Next Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjqawdXTJMLJ"
      },
      "source": [
        "# Reset for the Next Run\n",
        "\n",
        "train_writer.close()\n",
        "validation_writer.close()\n",
        "sess.close()\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2-0sMbyaPyK"
      },
      "source": [
        "# Keras Implementation\n",
        "\n",
        "Let's now dive into the implementation of our first neural network.\n",
        "Our network is a simple neural network, **without convolution operations**.\n",
        "\n",
        "We make use of the **sequential paradigm** of Tensorflow, made to build models by plugging together building blocks. This interface allows for easier code writing, while Tensorflow also offers alternative ways to write more complex deep learning algorithms through the use of its **define-by-run interface**.\n",
        "\n",
        "The network's structure is be the following :\n",
        " - A **flatten** layer, used to vectorize the whole input batch of data\n",
        " - A **dense** layer, transforming the 28x28=784 input data to a 512 vector, using a rectified linear unit activation function\n",
        " - A **dropout** layer, ensuring the network does not overfit the training data by giving each of its neuron a 20% chance not to be activated at each stage\n",
        " - A **dense** layer, outputing a 10 vector using a softmax function\n",
        "\n",
        "The optimizer we use at first is named **Adam**, because it requires very little parameter tuning.\n",
        "\n",
        "We use the sparse categorical crossentropy loss function because each sample of our data belongs to exactly one class (i.e. each handwritten digit represents only one specific digit).\n",
        "\n",
        "We also use the **accuracy** metric, which is basically the percentage of correct predictions our network computes.\n",
        "\n",
        "We will then train this neural network for **5 epochs** (i.e. on the whole dataset five times), and then test it on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXTfQGVwaRiM",
        "outputId": "7588b493-ef6d-4e4f-ce43-049bf0f4cd90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# Let's implement the network first\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Then choose the optimizer, loss function, and metric, as compilation parameters\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Let's now train the model we just built\n",
        "model.fit(X_train, Y_train, epochs=10)\n",
        "\n",
        "# And evaluate its performances on the testing set\n",
        "test_results = model.evaluate(X_test, Y_test)\n",
        "\n",
        "# Now finally print the value of the loss and metric functions specified above\n",
        "print(\"\\nloss :\", test_results[0])\n",
        "print(\"\\naccuracy :\", test_results[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 4.4797 - acc: 0.8402\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 12s 202us/sample - loss: 1.1167 - acc: 0.8949\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 1.0250 - acc: 0.9049\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 12s 207us/sample - loss: 1.0025 - acc: 0.9176\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 12s 207us/sample - loss: 0.9717 - acc: 0.9240\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 12s 197us/sample - loss: 0.9156 - acc: 0.9285\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 12s 196us/sample - loss: 0.9022 - acc: 0.9294\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 12s 204us/sample - loss: 0.9245 - acc: 0.9348\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 13s 216us/sample - loss: 0.8474 - acc: 0.9366\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 0.9088 - acc: 0.9377\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.7714 - acc: 0.9585\n",
            "\n",
            "loss : 0.7714054804242142\n",
            "\n",
            "accuracy : 0.9585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYM-pXOAnoGk"
      },
      "source": [
        "Some points and advices to check for students if you code is not working:\n",
        "1. Use constants - it will make you code more readible.\n",
        "2. It is important to do reshape AND rescale. For rescale you can divide by 255. or use normalization library, but be careful with it.\n",
        "3. Use one-hot encoding, it will improve your quality dramatically.\n",
        "4. Explain, why we nee train + validation + test split of data.\n",
        "5. Use proper placeholers with None.\n",
        "6. The folder for your tensorboard is './log'\n",
        "7. For starting data use stddev = 0.1 and aslo use names\n",
        "8. Use softmax_v2 loss with reduce mean - it should be number, not vector.\n",
        "9. Adam optimizer works like 10-20 times better that GradientDecsent\n",
        "10. Be careful with next_batch function: when data is finished you need to go back to the beginning.\n",
        "11. You have 10 classes, make sure you have a right function for correct_pred"
      ]
    }
  ]
}